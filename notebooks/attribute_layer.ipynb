{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dcb388",
   "metadata": {},
   "source": [
    "# ATTRIBUTE_LAYER.ipynb \n",
    "# Sparsity-Based/Value-Based Clustering Method for Hierarchy Engine\n",
    "\n",
    "---\n",
    "## Overview\n",
    "- Docstring and Imports\n",
    "- Column Selection Helper Function\n",
    "- Core Sparsity-Based Clustering Logic\n",
    "- Core Value-Based Clustering Logic\n",
    "- Public Assignment Method\n",
    "- Name Generation Purity Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce01e8",
   "metadata": {},
   "source": [
    "## 1 - Docstring and Imports\n",
    "The Attribute Layer utilizes sparsity-based clustering OR value-based clustering to create groups of items with similar sparsity patterns/attribute values. \n",
    "\n",
    "Sparsity-based clustering is especially effective with sparse, wide datasets (think 8,000+ columns). Example applications where sparsity-based clustering is effective includes product master data, term documents, and sensor networks. Within the context of Hierarchy, the Attribute Layer generates clusters in an attempt to group similar items within a greater category, allowing users to build more narrow item categories. \n",
    "\n",
    "The Attribute Layer is constructed using the sklearn library. Specifically, TruncatedSVD is used for dimensionality reduction to improve processing time, and KMeans is the actual clustering agent. NumPy and Pandas are used for data management, and annotations/type hints are incorporated for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc61bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# core/attribute_layer.py\n",
    "\n",
    "\"\"\"\n",
    "This file implements the Attribute Layer.\n",
    "\n",
    "The Attribute Layer groups rows within each category based on\n",
    "their attribute sparsity patterns (which columns are populated), and\n",
    "produces human-readable names for each attribute-based cluster.\n",
    "\n",
    "Public API used by the hierarchy engine:\n",
    "\n",
    "    assign_all_clusters(df, random_state=42) -> pd.DataFrame\n",
    "        Returns a copy of df with an integer \"attribute_cluster\"\n",
    "        column indicating the attribute-layer cluster id for each row.\n",
    "\n",
    "    make_cluster_names(df) -> (dict, pd.DataFrame)\n",
    "        Returns (cluster_name_map, df_with_names) where\n",
    "        \"attribute_cluster_name\" is added to the dataframe.\n",
    "\"\"\"\n",
    "\n",
    "# Type hints\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# External dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn dependencies\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122f05e",
   "metadata": {},
   "source": [
    "## 2 - Column Selection Helper\n",
    "This section covers the non-public attribute column selection method. It is a simple function that returns the attribute columns intended to be included into clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace323c",
   "metadata": {},
   "source": [
    "## 2.1 - Metadata Column Exclusion\n",
    "Specific metadata columns will be excluded from clustering. They tend to overpower the algorithm, and clusters built with these metadata columns are not particularly informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00f170",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Columns to exclude from attribute consideration\n",
    "_METADATA_COLS = {\n",
    "    \"category_name\",\n",
    "    \"category_cluster\",\n",
    "    \"category_cluster_name\",\n",
    "    \"attribute_cluster_id\",\n",
    "    \"attribute_cluster_name\",\n",
    "    \"level_0_id\",\n",
    "    \"level_0_name\",\n",
    "    \"level_1_id\",\n",
    "    \"level_1_name\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c3763",
   "metadata": {},
   "source": [
    "## 2.2 - Helper Function\n",
    "The actual function, `_select_attribute_columns()`, accepts two arguments: a Pandas DataFrame object `df`, and a list of strings `extra_excluded_cols`. In practice, `df` is the primary dataset the user intends to generate a hierarchy for, and `extra_excluded_cols` is an optional addional set of columns to not be considered in the clustering algorithm. The function returns a list of strings, which is the selection of attribute columns to be used for clustering. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- A set containing the metadata columns is stored in the variable `excluded`.\n",
    "- If `extra_excluded_cols` is not None, then `excluded` is refactored to contain the unique columns in `extra_excluded_cols`.\n",
    "- The return object `cols` is initialized as an empty list of strings.\n",
    "- Iterate through each column in the primary dataset.\n",
    "    - If the current column is in `excluded`, then continue to the next column\n",
    "    - If a column is a trivial/non-informative column such as `id` or `index`, then continue to the next column\n",
    "    - If the current column is completely null, then continue to the next column\n",
    "    - The current column is appended to the return object `cols` as a string\n",
    "- Return `cols`, the selected attribute columns (as a list of strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d98c36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to select attribute columns for clustering\n",
    "def _select_attribute_columns(\n",
    "    df: pd.DataFrame,\n",
    "    extra_excluded_cols: list[str] | None = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Heuristic to choose attribute columns for sparsity-based clustering.\n",
    "\n",
    "    We treat any non-all-null column that is not obviously part of the\n",
    "    hierarchy metadata as an attribute candidate, and also allow the caller\n",
    "    to explicitly exclude additional columns via `extra_excluded_cols`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df :\n",
    "        Primary DataFrame object for use in hierarchy generation\n",
    "    extra_excluded_cols :\n",
    "        Columns to exclude from clustering\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    List[str] :\n",
    "        An list of column names to be used as attribute columns\n",
    "    \"\"\"\n",
    "    excluded = set(_METADATA_COLS)\n",
    "    if extra_excluded_cols:\n",
    "        excluded |= set(map(str, extra_excluded_cols))\n",
    "\n",
    "    cols: List[str] = []\n",
    "    for col in df.columns:\n",
    "        if col in excluded:\n",
    "            continue\n",
    "        # Skip index-like or trivial columns\n",
    "        if str(col).lower() in {\"index\", \"id\"}:\n",
    "            continue\n",
    "        if df[col].isna().all():\n",
    "            continue\n",
    "        cols.append(col)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99e907",
   "metadata": {},
   "source": [
    "## 3 - Core Sparsity-Based Clustering Logic\n",
    "This section is responsible for the actual clustering method. The non-public function `_cluster_products_within_category` accepts three arguments: a Pandas DataFrame object `cat_df`, a list of strings `attr_cols`, and an integer `random_state`. In practice, `cat_df` is a subset of the primary dataset containing only the rows within the category of items to be clustered. Future versions of this project will generalize this logic so that attribute layers can be built using other attribute layers, instead of just item categories. Moving on, `attr_cols` is a list of columns to be considered as attributes for clustering. This object is the returned object from the previous helper function `_select_attribute_columns`. Lastly, `random_state` is used as the seed for the sklearn functions for reproducibility. The function returns a NumPy array of integer labels, corresponding to the cluster assignments for each row in `cat_df`. \n",
    "\n",
    "Due to the size of the function, it will be broken down into chunks of line-by-line explanations, rather than one giant breakdown. This first snippet is just the function signature and docstring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009c963",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _cluster_products_within_category_sparsity(\n",
    "    cat_df: pd.DataFrame,\n",
    "    attr_cols: List[str],\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster products within a single category using attribute sparsity\n",
    "    (which columns are non-null / non-empty).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_df :\n",
    "        Subset of the dataframe containing only rows for a single category.\n",
    "    attr_cols :\n",
    "        Columns to treat as attributes.\n",
    "    random_state :\n",
    "        Seed for KMeans and SVD.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of integer labels (cluster ids) with length len(cat_df).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406849b3",
   "metadata": {},
   "source": [
    "## 3.1 - Degenerate Guard\n",
    "This section is a failsafe to ensure that downstream code never crashes from empty arguments `attr_cols` and `cat_df`.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- If `attr_cols` does not exist OR `cat_df` is an empty dataframe, a single cluster \"0\" is created and every row is assigned to that cluster. The function returns this cluster assignment immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a4bd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    if not attr_cols or cat_df.empty:\n",
    "        # Degenerate: single cluster\n",
    "        return np.zeros(len(cat_df), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1aeb2b",
   "metadata": {},
   "source": [
    "## 3.2 - Boolean Mask\n",
    "This section initializes a boolen mask and populates it column by column.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- `mask` is initialized as a Pandas DataFrame, with a binary True/False assignment for each attribute column depending on if the column is null or not-null.\n",
    "- Iterate through each column in `attr_cols`.\n",
    "    - The contents of the current column are stored in the variable `col_series`.\n",
    "    - If `col_series` is type `object` (a string, empty string, or other kind of messy user-enteted data), then a boolean series is created and an attribute is marked present if it is not NaN, whitespace-only strings, or empty strings.\n",
    "    - If `col_series` is not type `object`, then a boolean series is created and an attribute is marked present if it is not NaN (the string-specific cases are not necessary here since this data is numeric/non-object).\n",
    "- The mask is converted into a NumPy array `X` with type `float` (True/False is now formatted as 1.0/0.0 since sklearn operates on NumPy arrays).\n",
    "- The dimensions of `X` are stored as `n_samples` (number of items in `cat_df`) and `n_features` (number of attribute columns).\n",
    "- If there are only 1 or 0 items in `cat_df`, a simple cluster assignment \"0\" is returned from this function as a guard.\n",
    "- If no item has any attribute populated (complete sparsity) then the same as above is returned since clustering is meaningless in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ac8ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Build a boolean mask: row x attribute_col\n",
    "    mask = pd.DataFrame(False, index=cat_df.index, columns=attr_cols)\n",
    "\n",
    "    for col in attr_cols:\n",
    "        col_series = cat_df[col]\n",
    "        if col_series.dtype == object:\n",
    "            mask[col] = col_series.notna() & (col_series.astype(str).str.strip() != \"\")\n",
    "        else:\n",
    "            mask[col] = col_series.notna()\n",
    "\n",
    "    # Convert to array\n",
    "    X = mask.to_numpy(dtype=float)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if n_samples <= 1:\n",
    "        return np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # If all-zero features, give up\n",
    "    if not np.any(X):\n",
    "        return np.zeros(n_samples, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dc4de",
   "metadata": {},
   "source": [
    "## 3.3 - K Heuristic\n",
    "This section determines the optimal k value for KMeans using the following heuristic: k is set to the squareroot of the number of items within the category. This value is clipped to prevent over-fragmentation and ensure that there is more than one cluster.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- The squareroot of `n_samples` is stored (rounded down to integer) as `k`, clipped such that the `k` is no less than 2 and no greater than 8. \n",
    "- If `k` is greater than `n_samples`, then it is set to be equal to `n_samples` (integer rounding sometime causes this).\n",
    "- If `k` is less than or equal to 1, then the single cluster assignment \"0\" is returned from this function (final guard for `k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f10ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  # Determine k using sqrt heuristic, clipped\n",
    "    k = int(np.clip(np.sqrt(n_samples), 2, 8))\n",
    "    if k > n_samples:\n",
    "        k = n_samples\n",
    "    if k <= 1:\n",
    "        return np.zeros(n_samples, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ef129",
   "metadata": {},
   "source": [
    "## 3.4 - TruncatedSVD Dimensionality Reduction\n",
    "This section reduces the dimensions of extremely wide data to improve KMeans performance.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- The number of dimensions `n_components` is set to be the minimum of 20, the value of `n_features` - 1, and the value of `n_samples` - 1 IF `n_features` is greater than 1 (more than 1 attribute column). Otherwise, `n_components` is set to 1.\n",
    "- If `n_components` is greater than or equal to 1, then the function attempts TruncatedSVD using `n_components` and `random_state`, storing it as `svd`. The result of `svd.fit_transform(X)` is stored as `X_reduced`.\n",
    "- If TruncatedSVD cannot be performed, then `X_reduced` is set equal to X (no reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27b8d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Dimensionality reduction for very wide data\n",
    "    n_components = min(20, n_features - 1, n_samples - 1) if n_features > 1 else 1\n",
    "    if n_components >= 1:\n",
    "        try:\n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "            X_reduced = svd.fit_transform(X)\n",
    "        except Exception:\n",
    "            # Fallback: no reduction\n",
    "            X_reduced = X\n",
    "    else:\n",
    "        X_reduced = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810f177",
   "metadata": {},
   "source": [
    "## NOTE - Why Truncated SVD?\n",
    "There are other dimensionality reduction techniques that can be considered, such as PCA and regular SVD. However, due to the structure of our data, these two methods are not optimal.\n",
    "\n",
    "**PCA**\n",
    "- Requires mean-centering\n",
    "- Densifies sparse data\n",
    "\n",
    "If a sparse binary dataset is mean-centered, it obstructs the boolean interpretation of the dataset through the introduction of artificial negative values. Additionally, when zero values are no long zero values, the dataset is no longer sparse. As a result, memory requirements skyrocket and computation speed slows down. Therefore, PCA is not an optimal technique for this scenario. \n",
    "\n",
    "**Regular SVD**\n",
    "- Creates exact reconstruction\n",
    "- Runs slow\n",
    "\n",
    "Because the pipeline only needs the top 10-20 dimensions for clustering, full SVD is overkill. Additionally, the time complexity, memory requirements, and numerical stability concerns make SVD both slow and risky.\n",
    "\n",
    "**Truncated SVD**\n",
    "- No mean-centering\n",
    "- No densifying matrices\n",
    "- No perfect reconstruction\n",
    "\n",
    "Truncated SVD's lack of centering/shifting helps to preserve the boolean interpretation of the data after reduction. Additionally, Truncated SVD does not densify the matrix in the process. Also, the objective of dimensionality reduction in this scenario is for clustering, not reconstruction. Therefore, Truncated SVD is able to get the job done with more speed and less memory requirements. The math will not be as \"pure\", but this design choice is for robustness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff14ee8",
   "metadata": {},
   "source": [
    "## 3.5 - KMeans Clustering\n",
    "This section houses the actual instance of KMeans clustering\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- The script attempts to perform k-means clustering using `n_clusters` for k. The method uses greedy k-means to make several trials at each sampling step, ensuring that the best centroids are chosen. (note: `n_init` simply equals `1` in this context). Following the `fit_predict` method, the resulting clustering assignments are stored in `labels`.\n",
    "- If k-means fails, then a single cluster is generated as a fallback using NumPy zero array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8af99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Cluster\n",
    "    try:\n",
    "        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "        labels = km.fit_predict(X_reduced)\n",
    "    except Exception:\n",
    "        # Fallback: single cluster\n",
    "        labels = np.zeros(n_samples, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d72a2",
   "metadata": {},
   "source": [
    "## 3.6 - Normalizing Labels and Return\n",
    "This section normalizes the resulting clustering assignments and returns the assignments.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Iterate through each label produced by clustering, ensure that the label is an integer, create a set of unique cluster IDs and sort the labels.\n",
    "- Map old labels to the new normalized labels.\n",
    "- Convert the mapping into a NumPy array.\n",
    "- Return the resulting clustering assignments.\n",
    "\n",
    "Note: Normalization is done per category since each category is clustered independently. The IDs are local instead of global, which is useful for naming and UI grouping. This is also one of the reasons why using `LabelEncoder` would be ineffective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddceff2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Normalize labels to 0..C-1 per category\n",
    "    unique_raw = sorted(set(int(l) for l in labels))\n",
    "    raw_to_new = {raw: i for i, raw in enumerate(unique_raw)}\n",
    "    labels_norm = np.array([raw_to_new[int(l)] for l in labels], dtype=int)\n",
    "\n",
    "    return labels_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834504d",
   "metadata": {},
   "source": [
    "## 4 - Core Value-Based Clustering Logic\n",
    "This section is responsible for an alternate clustering method, using attribute values instead of sparsity. The non-public function `_cluster_products_within_category_value` accepts three arguments: a Pandas DataFrame object `cat_df`, a list of strings `attr_cols`, and an integer `random_state`. In practice, `cat_df` is a subset of the primary dataset containing only the rows within the category of items to be clustered. Future versions of this project will generalize this logic so that attribute layers can be built using other attribute layers, instead of just item categories. Moving on, `attr_cols` is a list of columns to be considered as attributes for clustering. This object is the returned object from the previous helper function `_select_attribute_columns`. Lastly, `random_state` is used as the seed for the sklearn functions for reproducibility. The function returns a NumPy array of integer labels, corresponding to the cluster assignments for each row in `cat_df`. \n",
    "\n",
    "Due to the size of the function, it will be broken down into chunks of line-by-line explanations, rather than one giant breakdown. This first snippet is just the function signature and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb06ab1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _cluster_products_within_category_value(\n",
    "    cat_df: pd.DataFrame,\n",
    "    attr_cols: List[str],\n",
    "    random_state: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster products within a single category using attribute VALUES\n",
    "    (not just presence/absence).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_df :\n",
    "        Subset of the dataframe containing only rows for a single category.\n",
    "    attr_cols :\n",
    "        Columns to treat as attributes.\n",
    "    random_state :\n",
    "        Seed for KMeans and SVD.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of integer labels (cluster ids) with length len(cat_df).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be1ee1",
   "metadata": {},
   "source": [
    "## 4.1 - Degenerate Guard\n",
    "This section is a failsafe to ensure that downstream code never crashes from empty arguments `attr_cols` and `cat_df`.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- If `attr_cols` does not exist OR `cat_df` is an empty dataframe, a single cluster \"0\" is created and every row is assigned to that cluster. The function returns this cluster assignment immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64144ecb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    if not attr_cols or cat_df.empty:\n",
    "        return np.zeros(len(cat_df), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6327a",
   "metadata": {},
   "source": [
    "## 4.2 - Value-Based Matrix\n",
    "This section constructs the value-based matrix, the key difference between this function and the sparsity-based clustering function.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- `features` is initialized as an empty list\n",
    "- Iterate through each attribute column, storing the contents of the column as `s`, and checking if `s` is `NaN` for all values. If it is, then continue to the next column.\n",
    "- Otherwise, check to see if the column is numeric. If it is, fill the missing values using `s.median` to avoid skew. This is preferable to dropping rows, since dropping rows would break alignment and interfere with clustering (clustering requires equal-length rows). Then convert to a numeric column and append to the `features` list.\n",
    "- If the column is not numeric, then strings are converted to categorical codes using `pd.factorize`. `OneHotEncoder` is computationally slow and not preferred here because there are too many dimensions, and the dataset is extremely sparse. \n",
    "- In the case of non-numeric columns, \"missing\" is considered a separate code as a result of `codes.astype(float)`. This preserves the information that missing values provide. The codes are then appeneded to `features`.\n",
    "- If there are no usable features, then a single cluster is returned using the NumPy zero array. \n",
    "- Using `np.hstack`, features are combined into a matrix with dimensions \"items\" x \"attributes\" and the shape is extracted as `n_samples`, `n_features`.\n",
    "- If there are only 1 or 0 items in `cat_df`, a simple cluster assignment \"0\" is returned from this function as a guard.\n",
    "- If no item has any attribute populated (complete sparsity) then the same as above is returned since clustering is meaningless in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af11d93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    features = []\n",
    "    for col in attr_cols:\n",
    "        s = cat_df[col]\n",
    "        if s.isna().all():\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            filled = s.fillna(s.median())\n",
    "            features.append(filled.to_numpy(dtype=float).reshape(-1, 1))\n",
    "        else:\n",
    "            # Factorize string values\n",
    "            codes, _ = pd.factorize(s.astype(str), sort=True)\n",
    "            # Treat -1 (NA) as separate code\n",
    "            codes = codes.astype(float)\n",
    "            features.append(codes.reshape(-1, 1))\n",
    "\n",
    "    if not features:\n",
    "        return np.zeros(len(cat_df), dtype=int)\n",
    "\n",
    "    X = np.hstack(features)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if n_samples <= 1:\n",
    "        return np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    if not np.any(np.isfinite(X)):\n",
    "        return np.zeros(n_samples, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69465201",
   "metadata": {},
   "source": [
    "## 4.3 - K Heuristic\n",
    "This section determines the optimal k value for KMeans using the following heuristic: k is set to the squareroot of the number of items within the category. This value is clipped to prevent over-fragmentation and ensure that there is more than one cluster.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- The squareroot of `n_samples` is stored (rounded down to integer) as `k`, clipped such that the `k` is no less than 2 and no greater than 8. \n",
    "- If `k` is greater than `n_samples`, then it is set to be equal to `n_samples` (integer rounding sometime causes this).\n",
    "- If `k` is less than or equal to 1, then the single cluster assignment \"0\" is returned from this function (final guard for `k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a677367",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Determine k using sqrt heuristic, clipped\n",
    "    k = int(np.clip(np.sqrt(n_samples), 2, 8))\n",
    "    if k > n_samples:\n",
    "        k = n_samples\n",
    "    if k <= 1:\n",
    "        return np.zeros(n_samples, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2667754",
   "metadata": {},
   "source": [
    "## 4.4 - TruncatedSVD Dimensionality Reduction\n",
    "This section reduces the dimensions of extremely wide data to improve KMeans performance.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- The number of dimensions `n_components` is set to be the minimum of 20, the value of `n_features` - 1, and the value of `n_samples` - 1 IF `n_features` is greater than 1 (more than 1 attribute column). Otherwise, `n_components` is set to 1.\n",
    "- If `n_components` is greater than or equal to 1, then the function attempts TruncatedSVD using `n_components` and `random_state`, storing it as `svd`. The result of `svd.fit_transform(X)` is stored as `X_reduced`.\n",
    "- If TruncatedSVD cannot be performed, then `X_reduced` is set equal to X (no reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec41c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Optional dimensionality reduction for wide matrices\n",
    "    n_components = min(20, n_features - 1, n_samples - 1) if n_features > 1 else 1\n",
    "    if n_components >= 1:\n",
    "        try:\n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "            X_reduced = svd.fit_transform(X)\n",
    "        except Exception:\n",
    "            X_reduced = X\n",
    "    else:\n",
    "        X_reduced = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b62e80",
   "metadata": {},
   "source": [
    "## 4.5 - KMeans Clustering\n",
    "This section houses the actual instance of KMeans clustering\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- The script attempts to perform k-means clustering using `n_clusters` for k. The method uses greedy k-means to make several trials at each sampling step, ensuring that the best centroids are chosen. (note: `n_init` simply equals `1` in this context). Following the `fit_predict` method, the resulting clustering assignments are stored in `labels`.\n",
    "- If k-means fails, then a single cluster is generated as a fallback using NumPy zero array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10671d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    try:\n",
    "        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "        labels = km.fit_predict(X_reduced)\n",
    "    except Exception:\n",
    "        labels = np.zeros(n_samples, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ebab1",
   "metadata": {},
   "source": [
    "## 4.6 - Normalizing Labels and Return\n",
    "This section normalizes the resulting clustering assignments and returns the assignments.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Iterate through each label produced by clustering, ensure that the label is an integer, create a set of unique cluster IDs and sort the labels.\n",
    "- Map old labels to the new normalized labels.\n",
    "- Convert the mapping into a NumPy array.\n",
    "- Return the resulting clustering assignments.\n",
    "\n",
    "Note: Normalization is done per category since each category is clustered independently. The IDs are local instead of global, which is useful for naming and UI grouping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1cdb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Normalize labels to 0..C-1 per category\n",
    "    unique_raw = sorted(set(int(l) for l in labels))\n",
    "    raw_to_new = {raw: i for i, raw in enumerate(unique_raw)}\n",
    "    labels_norm = np.array([raw_to_new[int(l)] for l in labels], dtype=int)\n",
    "\n",
    "    return labels_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0cee8",
   "metadata": {},
   "source": [
    "## 5 - Public Assignment Method \n",
    "This section is responsible for running the clustering within the application by making use of private functions within the attribute layer script. The public function `assign_all_clusters` accepts three arguments: a Pandas DataFrame object `df`, an integer `random_state`, a list of strings `extra_excluded_cols`, and a string `method`. In practice, `df` is the complete primary dataset. Next, `random_state` is used as the seed for the sklearn functions for reproducibility. Moving on, `extra_excluded_cols` is a list of columns to be excluded from consideration as attributes for clustering. Lastly, `method` is a string that declares whether sparsity-clustering or value-clustering is used. The function returns a Pandas DataFrame of the original dataset with cluster assignment labels for each row.\n",
    "\n",
    "Due to the size of the function, it will be broken down into chunks of line-by-line explanations, rather than one giant breakdown. This first snippet is just the function signature and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a47f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def assign_all_clusters(\n",
    "    df: pd.DataFrame,\n",
    "    random_state: int = 42,\n",
    "    extra_excluded_cols: list[str] | None = None,\n",
    "    method: str = \"sparsity\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign attribute-layer clusters within each category.\n",
    "\n",
    "    For every distinct value of `category_name`, we cluster its products\n",
    "    using either:\n",
    "\n",
    "        - 'sparsity': attribute sparsity patterns (which columns are present)\n",
    "        - 'value':    attribute values (numeric + factorized categorical)\n",
    "\n",
    "    and assign an integer `attribute_cluster` id (0..K-1 for that category).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df :\n",
    "        Input dataframe. Must contain `category_name`.\n",
    "    random_state :\n",
    "        Random seed for clustering.\n",
    "    extra_excluded_cols :\n",
    "        Optional list of column names to exclude from attribute clustering\n",
    "        (in addition to the built-in metadata exclusions).\n",
    "    method :\n",
    "        'sparsity' or 'value'.\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        A dataframe of the original dataset with cluster assignments as integers, and a shape of the original dataset plus an additional column.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b1a93",
   "metadata": {},
   "source": [
    "# 5.1 - Validation Check\n",
    "\n",
    "This section is a simple guard to ensure that the column `category_name` exists in the dataset. If it does not, the function fails fast with a clear error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3e95c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    if \"category_name\" not in df.columns:\n",
    "        raise ValueError(\"Expected column 'category_name' in dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f553cca",
   "metadata": {},
   "source": [
    "# 5.2 - Column Selection\n",
    "\n",
    "This section runs the function `_select_attribute_columns` using the input dataset and the specified excluded columns, and stores the return as `attr_cols`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb24917",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    attr_cols = _select_attribute_columns(df, extra_excluded_cols=extra_excluded_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c4c95",
   "metadata": {},
   "source": [
    "# 5.3 - Clustering and Return\n",
    "\n",
    "This section executes the actual clustering functions. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- A copy of `df` is stored as `df_out` to avoid mutating the original dataset itself.\n",
    "- Cluster assignments are initialzed prior to clustering using the NumPy zero array.\n",
    "- Iterate through each category in the dataset, grouping them together and storing the current slice as `cat_df`.\n",
    "- Check to see which clustering method was passed into the function (sparsity vs. value)\n",
    "- If clustering using value, then the return of `_cluster_products_within_category_value` (using `cat_df`, `attr_cols`, and `random_state` as arguments) is stored as `labels`. If clustering using sparsity, then the return of `_cluster_products_within_category_sparsity` (using `cat_df`, `attr_cols`, and `random_state` as arguments) is stored as `labels`.\n",
    "- At the end of each iteration, the category-specific clustering labels are written back into the global array as `all_labels`, making sure that each row (item) is assigned the correct cluster label. \n",
    "- After each category has been clustered, the column `attribute_cluster` for `df_out` is generated using `all_labels` and the resulting dataframe is returned.\n",
    "\n",
    "Note: This function runs sparsity-based clustering by default, and the iteration only checks for `value`. As a result, in the event of a typo, invalid entry, or missing argument, the function will run sparsity-based clustering to prevent crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0def6ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    df_out = df.copy()\n",
    "    all_labels = np.zeros(len(df_out), dtype=int)\n",
    "\n",
    "    for cat, cat_idx in df_out.groupby(\"category_name\").groups.items():\n",
    "        cat_df = df_out.loc[cat_idx]\n",
    "\n",
    "        if method == \"value\":\n",
    "            labels = _cluster_products_within_category_value(\n",
    "                cat_df,\n",
    "                attr_cols,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            labels = _cluster_products_within_category_sparsity(\n",
    "                cat_df,\n",
    "                attr_cols,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "\n",
    "        all_labels[cat_df.index.to_numpy()] = labels\n",
    "\n",
    "    df_out[\"attribute_cluster\"] = all_labels\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f775a26",
   "metadata": {},
   "source": [
    "# 6 - Name Generation Purity Method\n",
    "\n",
    "This section covers the naming algorithm used throughout the application. It is primarily used for creating human-readable names for attribute-layer clusters, but it is also implemented within the category-layer for producing synthetic categories when the dataset does not have a category system in place. The public function `make_cluster_names` accepts three arguments: a Pandas DataFrame object `df`, a float `purity_threshold`, and a list of strings `extra_excluded_cols`. In practice, `df` is the complete primary dataset. Next, `purity_threshold` is a value determining whether the achieved purity score is significant enough to consider the corresponding attribute column to be a descriptive name. Lastly, `extra_excluded_cols` is a list of columns to be excluded from consideration as attributes for clustering. The function returns the complex data structure `Tuple[Dict[tuple, str], pd.DataFrame]`, which is the category-cluster pair mapped to the corresponding cluster label, coupled with the resulting dataframe that includes the category-cluster name. \n",
    "\n",
    "Due to the size of the function, it will be broken down into chunks of line-by-line explanations, rather than one giant breakdown. This first snippet is just the function signature and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49422c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def make_cluster_names(\n",
    "    df: pd.DataFrame,\n",
    "    purity_threshold: float = 0.5,\n",
    "    extra_excluded_cols: list[str] | None = None,\n",
    ") -> Tuple[Dict[tuple, str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute human-readable names for attribute-layer clusters.\n",
    "\n",
    "    For each (category_name, attribute_cluster) group, we scan the\n",
    "    attribute columns and look for columns whose values are relatively\n",
    "    pure within the cluster (most rows share the same non-null value).\n",
    "\n",
    "    We then generate a label such as:\n",
    "\n",
    "        \"Binding Covers – 210 (Width mm) / 16 (Height mm)\"\n",
    "\n",
    "    where the pieces are taken from high-purity attribute values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df :\n",
    "        Input dataframe. Must contain 'category_name' and 'attribute_cluster'.\n",
    "    purity_threshold :\n",
    "        Minimum fraction of rows within a cluster that must share the\n",
    "        same value in an attribute column for that (column, value)\n",
    "        descriptor to be used in the label.\n",
    "    extra_excluded_cols :\n",
    "        Optional list of columns to exclude from naming consideration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[tuple, str], pd.DataFrame] :\n",
    "        Category-cluster pair mapped to the corresponding cluster label, coupled with the resulting dataframe that includes the category-cluster name.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f7b95",
   "metadata": {},
   "source": [
    "# 6.1 - Validation Check\n",
    "\n",
    "This section is a simple guard to ensure that the columns `category_name` and `attribute_cluster` exist in the dataset. If either one does not, the function fails fast with a clear error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418403e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    if \"category_name\" not in df.columns or \"attribute_cluster\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'category_name' and 'attribute_cluster' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a0629",
   "metadata": {},
   "source": [
    "# 6.2 - Column Selection\n",
    "\n",
    "This section runs the function `_select_attribute_columns` using the input dataset and the specified excluded columns, and stores the return as `attr_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfd309",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    attr_cols = _select_attribute_columns(df, extra_excluded_cols=extra_excluded_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c1edd",
   "metadata": {},
   "source": [
    "# 6.3 - Mapping\n",
    "\n",
    "This section creates a safe copy of the original dataframe, initializes the cluster map as a dictionary with tuples (category_name, category_cluster) mapped to strings (labels), and initializes the `attribute_cluster_name` column in the dataframe copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d390c8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    df_out = df.copy()\n",
    "    cluster_name_map: Dict[tuple, str] = {}\n",
    "    df_out[\"attribute_cluster_name\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc6b30",
   "metadata": {},
   "source": [
    "# 6.4 Degenerate Guard\n",
    "\n",
    "This section is a failsafe for missing attribute columns. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- If attribute columns are missing, then iterate through each cluster within each category.\n",
    "- A misc. label is assigned to the current cluster within the category and is stored as `label`.\n",
    "- The label is mapped to `cluster_name_map` and written into the dataframe.\n",
    "- After iteration has concluded, the mapping and resulting dataframe are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69433e54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    if not attr_cols:\n",
    "        # Degenerate: just use category name + generic suffix\n",
    "        for (cat, cid), idx in df_out.groupby(\n",
    "            [\"category_name\", \"attribute_cluster\"]\n",
    "        ).groups.items():\n",
    "            label = f\"{cat} – misc\"\n",
    "            cluster_name_map[(cat, cid)] = label\n",
    "            df_out.loc[idx, \"attribute_cluster_name\"] = label\n",
    "        return cluster_name_map, df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf2a56",
   "metadata": {},
   "source": [
    "# 6.5 - Naming Algorithm\n",
    "\n",
    "This section contains the actual purity-based naming algorithm, returning the resulting map and dataframe.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- A set of strings indicating missing values are stored in `nan_like`. This set helps exclude uninformative missing values from the naming process, preventing names such as `None (Color)`.\n",
    "- The outer loop begins, iterating through each category-cluster pair. The number of rows within the pair (accessed here using the corresponding subset dataframe `grp`) is stored as `n_rows`, and the list of significant label fragments is initialzed as `descriptors`.\n",
    "- The inner loop begins, iterating through each attribute column individually. Column values for the current cluster is stored as `col_series`, skipping the current column is the values are all null. If there are not all null, then the values are normalized and stored `vals`, which then has any empty strings snf nan-like strings dropped. At this point, if `vals` is empty, the current column is skipped.\n",
    "- Within the inner loop, each distinct value is counted and stored as `vc`. The most common value is stored as `top_val`, and the count of this value is stored as `top_count`. The purity score is evaluated and stored as `top_count` divided by `n_rows`, which is the fraction of rows within the cluster that contain this value. \n",
    "- If the calculated purity score achieves or exceeds the `purity_threshold` parameter, then the descriptor is stored and the current iteration of the inner loop concludes.\n",
    "- Returning to the outer loop, if `descriptors` actually contains descriptors, then the label is generated using the three most popular values. Otherwise, a misc. label is generated instead. The map and dataframe are updated for the current category-cluster pair, and the next iteration begins.\n",
    "- Upon the conclusion of the outer loop, the resulting map and dataframe are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f462484",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Treat these string values as \"missing\" when naming\n",
    "    nan_like = {\"nan\", \"none\", \"null\", \"na\", \"n/a\"}\n",
    "\n",
    "    for (cat, cid), grp in df_out.groupby([\"category_name\", \"attribute_cluster\"]):\n",
    "        n_rows = len(grp)\n",
    "        descriptors: List[str] = []\n",
    "\n",
    "        # Scan each attribute column for high-purity values\n",
    "        for col in attr_cols:\n",
    "            col_series = grp[col]\n",
    "\n",
    "            # Skip if all null\n",
    "            if col_series.isna().all():\n",
    "                continue\n",
    "\n",
    "            vals = col_series.astype(str).str.strip()\n",
    "\n",
    "            # Drop empty strings\n",
    "            vals = vals[vals != \"\"]\n",
    "\n",
    "            # Drop nan-like string representations\n",
    "            vals = vals[~vals.str.lower().isin(nan_like)]\n",
    "\n",
    "            if vals.empty:\n",
    "                continue\n",
    "\n",
    "            vc = vals.value_counts(dropna=True)\n",
    "            top_val = vc.index[0]\n",
    "            top_count = vc.iloc[0]\n",
    "            purity = top_count / n_rows\n",
    "\n",
    "            if purity >= purity_threshold:\n",
    "                descriptors.append(f\"{top_val} ({col})\")\n",
    "\n",
    "        if descriptors:\n",
    "            desc_str = \" / \".join(descriptors[:3])\n",
    "            label = f\"{cat} – {desc_str}\"\n",
    "        else:\n",
    "            label = f\"{cat} – misc\"\n",
    "\n",
    "        cluster_name_map[(cat, cid)] = label\n",
    "        df_out.loc[grp.index, \"attribute_cluster_name\"] = label\n",
    "\n",
    "    return cluster_name_map, df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5476c0",
   "metadata": {},
   "source": [
    "# NOTE - Why Purity-Based?\n",
    "\n",
    "Purity Scores are effective for generating names based on the defining attributes of a cluster. Since many master datasets are sparse and wide, the presence of specific columns (and the actual entries for those columns) are significant for identifying natural groupings of items.\n",
    "\n",
    "Another option to consider is TF-IDF naming, which was utilized in an earlier version of this project. In theory, TF-IDF would be a powerful upgrade. Having clear, significant, and grammatically correct cluster names would be valuable, and in certain text-based datasets with low sparsity and uniform text column formats, TF-IDF would perform well.\n",
    "\n",
    "However, TF-IDF struggled in a few key areas in this project:\n",
    "- Managing unstructured text data columns\n",
    "- Identifying most popular values within a group\n",
    "- Handling sparsity \n",
    "\n",
    "As a result, TF-IDF often produced non-sensible or insignicant names for many clusters. The goal of naming the clusters is to be able to interpret how the clusters are grouped. Purity scores identify the most significant descriptors in each cluster as well as the most common value, which helps a human understand exactly what types of items the cluster contains. Thus, the project now implements purity-based naming instead of TF-IDF."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
