{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dcb388",
   "metadata": {},
   "source": [
    "# TEXT_UTILS.ipynb \n",
    "# Utility Functions for Text Data Management\n",
    "\n",
    "---\n",
    "## Overview\n",
    "- Docstring and Imports\n",
    "- Globals\n",
    "- Helper Functions\n",
    "- Embeddings for Labels\n",
    "- TF-IDF Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce01e8",
   "metadata": {},
   "source": [
    "## 1 - Docstring and Imports\n",
    "The core logic of this application leverages the text-based utility functions outlined in this notebook. \n",
    "\n",
    "Regex is used for data cleaning, NumPY is used for data management, Counter is used for the `word_counts` object, and annotations/type hints are incorporated for clarity. Additionally, TfidVectorizer and SentenceTransformer handle text-data processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c73cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# core/text_utils.py\n",
    "\n",
    "\"\"\"\n",
    "Text utilities for semantic and attribute clustering.\n",
    "\n",
    "This module provides:\n",
    "\n",
    "    - A shared SentenceTransformer model for embeddings\n",
    "    - Helpers for normalizing and tokenizing text\n",
    "    - TF-IDF–based cluster labeling that is robust to sparse input\n",
    "\"\"\"\n",
    "\n",
    "# Type hints\n",
    "from __future__ import annotations\n",
    "from typing import List, Sequence\n",
    "\n",
    "# External dependencies\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn dependencies\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sentence Transformer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7dde9",
   "metadata": {},
   "source": [
    "# 2 - Globals\n",
    "\n",
    "This section contains a shared model, as well as the private function `_get_sentence_model` that accepts a string `model_name` (desired model to use).\n",
    "\n",
    "The private variable `_sentence_model` is a module-level variable that starts as `None`. \n",
    "\n",
    "Line-by-line breakdown of the function:\n",
    "- Consider `_sentence_model` to be global.\n",
    "- Check to see if the model exists. If it does not, then the desired model is loaded, stored, and returned. If not, then there is no reload and the model is immediately returned.\n",
    "\n",
    "The purpose of the function is to ensure that the desired model is loaded only once. `SentenceTransformer` is slow and memory-heavy, taking a long time to run. It does not need to reload every time text is embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcea28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "_sentence_model: SentenceTransformer | None = None\n",
    "\n",
    "def _get_sentence_model(model_name: str = \"all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Lazily load and cache the SentenceTransformer model used for\n",
    "    semantic embedding of labels (categories, cluster names, etc.).\n",
    "    \"\"\"\n",
    "    global _sentence_model\n",
    "    if _sentence_model is None:\n",
    "        _sentence_model = SentenceTransformer(model_name)\n",
    "    return _sentence_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7666a68",
   "metadata": {},
   "source": [
    "# 3 - Helper Functions\n",
    "\n",
    "This section contains two helper functions for text normalization and tokenization. Each function will be broken down separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbebe3",
   "metadata": {},
   "source": [
    "# 3.1 - Normalize\n",
    "\n",
    "This function accepts a string `text` and returns a string. In practice, the input string is any desired text to normalize, and the output is the same string without whitespace. This function ensures that empty strings are not used for TF-IDF or embedding.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Ensure input text is in fact a string.\n",
    "- Replace any whitespace with a single space.\n",
    "- Strip whitespace and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605711c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize whitespace and coerce to string.\n",
    "\n",
    "    This is intentionally lightweight: we primarily use it to make\n",
    "    sure we don't feed empty strings or pathological whitespace into\n",
    "    TF-IDF or embedding models.\n",
    "    \"\"\"\n",
    "    s = str(text)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a241154",
   "metadata": {},
   "source": [
    "# 3.2 - Tokenize\n",
    "\n",
    "This function accepts a string `text` and returns a list of strings. In practice, the input string is any desired text to tokenize, and the output is the tokens of the string. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Use `normalize_text` to normalize the input and force to lowercase.\n",
    "- Get any digit or letter character that is one or more characters long, and return all matching substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cbfed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very simple tokenizer for fallback frequency calculations.\n",
    "\n",
    "    - Lowercase\n",
    "    - Extract sequences of letters/digits as tokens\n",
    "    \"\"\"\n",
    "    s = normalize_text(text).lower()\n",
    "    return re.findall(r\"\\b\\w+\\b\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea5430",
   "metadata": {},
   "source": [
    "# 4 - Embeddings for Labels\n",
    "\n",
    "The public function `build_embeddings_for_labels` is the crutch of the Semantic Layer. The function accepts a Sequence of strings `labels` (category names or cluster labels to embed), and returns a NumPy array containing the embeddings. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Convert sequence into a list so that it can be iterated.\n",
    "- If `labels` does not exist, then early return an empty NumPy array.\n",
    "- `get_sentence_model()` retrieves the desired model and stores it as `model`.\n",
    "- Run `normalize_text` on each label and store all normalized lables as a list `cleaned`.\n",
    "- Encode cleaned labels and store the result as `emb`.\n",
    "- Return the embeddings as a NumPy array of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0f4b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_embeddings_for_labels(labels: Sequence[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build sentence-level embeddings for a sequence of label strings,\n",
    "    using a shared SentenceTransformer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels :\n",
    "        Any iterable of strings (e.g., category names, cluster labels).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        2D array of shape (len(labels), embedding_dim).\n",
    "        Returns an empty array if `labels` is empty.\n",
    "    \"\"\"\n",
    "    labels = list(labels)\n",
    "    if not labels:\n",
    "        return np.zeros((0, 0), dtype=float)\n",
    "\n",
    "    model = _get_sentence_model()\n",
    "    cleaned = [normalize_text(x) for x in labels]\n",
    "    emb = model.encode(cleaned, show_progress_bar=False)\n",
    "    return np.asarray(emb, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855cb56",
   "metadata": {},
   "source": [
    "# 5 - TF-IDF Clustering\n",
    "\n",
    "The public function `tfidf_cluster_label` is also integral to the Semantic Layer, specifically for naming. The function takes a Sequence of strings `texts`, an integer `max_words`, an integer `min_df`, and a float `max_df`. In practice, `texts` is the strings of items to be used for creating labels, `max_words` is the upper limit of how many words can be included in the name, and `min_df` and `max_df` are the lower and upper limits on how frequent a token must appear in a document to be considered. The function returns a string, which is the human-readable label calculated using TF-IDF scores. \n",
    "\n",
    "Due to its size and complexity, it will broken down into chunks. This first section is the docstring and signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6558ae3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_cluster_label(\n",
    "    texts: Sequence[str],\n",
    "    max_words: int = 4,\n",
    "    min_df: int = 1,\n",
    "    max_df: float = 0.9,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compute a short, human-readable label from a collection of texts\n",
    "    using TF-IDF scores.\n",
    "\n",
    "    This is used to name semantic clusters in a way that\n",
    "    reflects the most informative terms appearing in cluster members.\n",
    "\n",
    "    The function is designed to be robust:\n",
    "      - If input is empty → returns \"misc\"\n",
    "      - If TF-IDF pruning removes all terms → relaxes pruning and retries\n",
    "      - If that still fails → falls back to simple token frequency\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts :\n",
    "        Sequence of strings belonging to a single cluster.\n",
    "    max_words :\n",
    "        Maximum number of words to include in the label.\n",
    "    min_df :\n",
    "        Minimum document frequency for TF-IDF features.\n",
    "    max_df :\n",
    "        Maximum document frequency (as a proportion) for TF-IDF features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A title-cased cluster label, or \"misc\" if no good label\n",
    "        can be determined.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fbf89",
   "metadata": {},
   "source": [
    "# 5.1 - Normalize and Filter\n",
    "\n",
    "This section calls `normalize_text` for each text and stores the results a list `docs`. If `docs` is empty, then the function early returns the label `misc` as a failsafe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ec933",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    docs = [normalize_text(t) for t in texts if normalize_text(t)]\n",
    "    if not docs:\n",
    "        return \"misc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86460e8d",
   "metadata": {},
   "source": [
    "# 5.2 - Vectorization\n",
    "\n",
    "This section performs the actual vectorization. \n",
    "\n",
    "Line-by-line breakdown:\n",
    "- A TfidVectorizer `vec` is initialized \n",
    "- The function tries to fit the normalized text using the vector and store it as `X`. If it fails, the function tries it again with looser pruning requirements. \n",
    "- If it fails again, then a failsafe is triggered, beginning with the initializatoin of a list of strings `tokens`.\n",
    "- Each document is tokenized using the helper function `tokenize` and added to `tokens`.\n",
    "- If there are no valid tokens, then the `misc` fallback is returned.\n",
    "- Otherwise, the total number of tokens is stored as `counts`.\n",
    "- The most common tokens are stored as `top_tokens`.\n",
    "- The most common tokens are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84c382",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # First pass: user-specified min_df / max_df\n",
    "    vec = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        token_pattern=r\"\\b\\w+\\b\",\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        X = vec.fit_transform(docs)\n",
    "    except ValueError:\n",
    "        # Typical case: \"After pruning, no terms remain\".\n",
    "        # Relax pruning and retry.\n",
    "        vec = TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            token_pattern=r\"\\b\\w+\\b\",\n",
    "            min_df=1,\n",
    "            max_df=1.0,\n",
    "        )\n",
    "        try:\n",
    "            X = vec.fit_transform(docs)\n",
    "        except Exception:\n",
    "            # Final fallback: simple frequency over tokens\n",
    "            tokens: List[str] = []\n",
    "            for d in docs:\n",
    "                tokens.extend(tokenize(d))\n",
    "            if not tokens:\n",
    "                return \"misc\"\n",
    "            counts = Counter(tokens)\n",
    "            top_tokens = [w for w, _ in counts.most_common(max_words)]\n",
    "            return \" \".join(top_tokens).title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca6aea",
   "metadata": {},
   "source": [
    "# 5.3 - Additional Misc Fallbacks\n",
    "\n",
    "This section contains a couple of fallbacks, as well as initializing `terms` and `scores`.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- If there is no vocabulary present in `X`, then return `misc`.\n",
    "- Calculate `terms`, a NumPy array of vocabulary words.\n",
    "- Calculate `scores`, a NumPy array of average TF-IDF scores corresponding to each term in the documents\n",
    "- If there are no `scores`, then return `misc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321b3f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    if X.shape[1] == 0:\n",
    "        # No features survived\n",
    "        return \"misc\"\n",
    "\n",
    "    terms = np.array(vec.get_feature_names_out())\n",
    "    scores = np.asarray(X.mean(axis=0)).ravel()\n",
    "    if scores.size == 0:\n",
    "        return \"misc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd30fe5",
   "metadata": {},
   "source": [
    "# 5.4 - Ranking\n",
    "\n",
    "This section, ranks terms by average TF-IDF score, while containing a few more fallbacks.\n",
    "\n",
    "Line-by-line breakdown:\n",
    "- Rank the terms by score in descending order.\n",
    "- Initialize `label_tokens`, a list of strings.\n",
    "- Iterate through each index of the ordered terms. If the score of the index is less than or equal to 0, continue to the next index. Otherwise, take the term whose TF-IDF score ranks at this position, and add the actual word string to the label.\n",
    "- If the length of `label_tokens` is less than or equal to the maximum desired length, then stop iterating.\n",
    "- If there are no labels, then return `misc`.\n",
    "- Return a pretty version of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6dd44f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    label_tokens: List[str] = []\n",
    "    for idx in order:\n",
    "        if scores[idx] <= 0:\n",
    "            continue\n",
    "        label_tokens.append(terms[idx])\n",
    "        if len(label_tokens) >= max_words:\n",
    "            break\n",
    "\n",
    "    if not label_tokens:\n",
    "        return \"misc\"\n",
    "\n",
    "    # Title-case to look nicer as a cluster name\n",
    "    return \" \".join(label_tokens).title()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
